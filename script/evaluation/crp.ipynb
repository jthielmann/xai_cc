{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:08.614746Z",
     "start_time": "2024-09-12T08:43:08.581834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.attribution import CondAttribution\n",
    "from zennit.attribution import Gradient\n",
    "from zennit.canonizers import CompositeCanonizer\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.visualization import FeatureVisualization\n",
    "from crp.image import plot_grid\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "import zennit as zen\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import zennit.torchvision as ztv\n",
    "from crp.image import imgify\n",
    "\n",
    "\n",
    "from relevance import plot_relevance\n",
    "from model import get_vggs_and_path, get_resnets_and_path, get_remote_models_and_path\n",
    "from plot_and_print import plot_tile\n",
    "from data_loader import TileLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from data_loader import get_data_loaders, get_dataset, STDataset"
   ],
   "id": "48e3d1fbc8de2692",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:09.242784Z",
     "start_time": "2024-09-12T08:43:09.238256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_heatmaps(imgs, image_path, width=4, subplot_size=30):\n",
    "    # +1 because we have the original tile plotted as extra image and +0.999 because we want to round up in case it is not perfectly dividable\n",
    "    height = int((len(imgs) + 1) / width + 0.999)\n",
    "    plt.figure(figsize=(width * subplot_size, height * subplot_size))\n",
    "\n",
    "    f, ax = plt.subplots(height, width)\n",
    "    f.set_figheight(subplot_size)\n",
    "    f.set_figwidth(subplot_size)\n",
    "    for i in range(len(imgs)):\n",
    "        ax[int(i / width), i % width].imshow(imgs[i][0])\n",
    "        ax[int(i / width), i % width].set_title(imgs[i][1])\n",
    "        ax[int(i / width), i % width].axis('off')\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    ax[-1, -1].imshow(img)\n",
    "    ax[-1, -1].set_title('original')\n",
    "    ax[-1, -1].axis('off')\n",
    "    plt.show()"
   ],
   "id": "80fc79f083d44e40",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:14.617986Z",
     "start_time": "2024-09-12T08:43:09.903945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = get_remote_models_and_path()\n",
    "loader = TileLoader()\n",
    "\n",
    "tile_path = \"../Test_Data/p026/tiles/p026_11_60.tiff\"\n",
    "\n",
    "data = loader.open(tile_path).unsqueeze(0)\n",
    "target = 2.580166\n",
    "data.requires_grad_(True)\n",
    "plot_tile(tile_path)"
   ],
   "id": "820bc5a8d0f42563",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:15.661572Z",
     "start_time": "2024-09-12T08:43:15.656689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = models[3][0]\n",
    "print(models[3][1])\n",
    "composite_res = zen.composites.EpsilonPlusFlat(canonizers=[ztv.ResNetCanonizer()])\n",
    "# TODO: check if cannonizer uses basic block / bottle neck\n",
    "\n",
    "\n",
    "\n",
    "# is either torchvision.models.resnet.BasicBlock or \n",
    "# torchvision.models.resnet.Bottleneck\n",
    "bottleneck_type = model.encoder.layer1[0].__class__\n",
    "print(bottleneck_type)"
   ],
   "id": "4f8f14cb7c5a25f0",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:18.280792Z",
     "start_time": "2024-09-12T08:43:18.275564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def plot_heatmaps(imgs, image_path, width=4, subplot_size=4):\n",
    "    # Calculate the height of the plot grid\n",
    "    height = int((len(imgs) + 1) / width + 0.999)\n",
    "    \n",
    "    # Set the figure size (width * subplot_size, height * subplot_size)\n",
    "    f, ax = plt.subplots(height, width, figsize=(width * subplot_size, height * subplot_size), constrained_layout=True)\n",
    "    \n",
    "    # Loop to display images\n",
    "    for i in range(len(imgs)):\n",
    "        ax[int(i / width), i % width].imshow(imgs[i][0])\n",
    "        ax[int(i / width), i % width].set_title(imgs[i][1])\n",
    "        ax[int(i / width), i % width].axis('off')  # Optionally remove axes for a cleaner look\n",
    "\n",
    "    # Plot the last image\n",
    "    img = Image.open(image_path)\n",
    "    ax[-1, -1].imshow(img)\n",
    "    ax[-1, -1].set_title('original')\n",
    "    ax[-1, -1].axis('off')  # Optionally remove axes for a cleaner look\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example call to the function (assuming `imgs` and `image_path` are defined)\n",
    "# plot_heatmaps(imgs, image_path)\n"
   ],
   "id": "c069bc8e4d1098ca",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:20.482589Z",
     "start_time": "2024-09-12T08:43:19.836863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attribution = CondAttribution(model)\n",
    "\n",
    "# here, each channel is defined as a concept\n",
    "# or define your own notion!\n",
    "cc = ChannelConcept()\n",
    "\n",
    "# get layer names of Conv2D and MLP layers\n",
    "layer_names = get_layer_names(model, [bottleneck_type])\n",
    "print(layer_names)\n",
    "# get a conditional attribution for channel 50 in layer features.27 wrt. output 1\n",
    "#conditions = [{\"y\":[0], \"gene1.0\": [35]}]\n",
    "\n",
    "heatmaps = []\n",
    "\"\"\"\n",
    "for layer in layer_names:\n",
    "    conditions = [{\"y\":[0], layer:[0]}]\n",
    "    \n",
    "    attr = attribution(data, conditions, composite_res, record_layer=layer_names)\n",
    "    heatmaps.append((attr.heatmap.squeeze(), layer))\n",
    "\"\"\"\n",
    "\n",
    "for i in range (0):\n",
    "    conditions = [{\"y\":[0], \"encoder.layer4.1\":[i]}]\n",
    "    \n",
    "    attr = attribution(data, conditions, composite_res, record_layer=layer_names)\n",
    "    heatmaps.append((attr.heatmap.squeeze(), \"encoder.layer4.1 \" + str(i)))\n",
    "\n",
    "with Gradient(model, composite_res) as attributor:\n",
    "    out, grad = attributor(data)\n",
    "rel = plot_relevance(grad, filename=None, only_return=True)\n",
    "heatmaps.append((rel, \"unfiltered LRP\"))\n",
    "\n",
    "plot_heatmaps(heatmaps, tile_path, 4)\n",
    "\n",
    "\n",
    "# heatmap and prediction\n",
    "#attr.heatmap, attr.prediction\n",
    "# activations and relevances for each layer name\n",
    "#attr.activations, attr.relevances\n",
    "\n",
    "# relative importance of each concept for final prediction\n",
    "rel_c = cc.attribute(attr.relevances['encoder.layer1.0'])\n",
    "concept_ids = torch.argsort(rel_c, descending=True)"
   ],
   "id": "5b62a682e12692db",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ],
   "id": "379f91a7ec28873f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T21:34:14.545669Z",
     "start_time": "2024-09-10T21:34:14.539676Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "9afba68f5c2c5174",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:26.503795Z",
     "start_time": "2024-09-12T08:43:25.808592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.helper import get_layer_names\n",
    "\n",
    "layer_names = get_layer_names(model, [bottleneck_type])\n",
    "\n",
    "conditions = [{'y': [0]}]\n",
    "attr = attribution(data, conditions, composite_res, record_layer=layer_names)\n",
    "\n",
    "print(attr.activations['encoder.layer4.1'].shape, attr.relevances['encoder.layer4.1'].shape)\n",
    "# attr[1][\"features.40\"].shape, attr[2][\"features.40\"].shape # is equivalent\n",
    "# layer features.40 has 512 channel concepts\n",
    "rel_c = cc.attribute(attr.relevances['encoder.layer4.1'], abs_norm=True)\n",
    "print(rel_c.shape)\n",
    "# the six most relevant concepts and their contribution to final classification in percent\n",
    "rel_values, concept_ids = torch.topk(rel_c[0], 6)\n",
    "concept_ids, rel_values*100\n",
    "\n"
   ],
   "id": "4f7cb83a270dfd4b",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:43:44.600148Z",
     "start_time": "2024-09-12T08:43:43.856898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(concept_ids)\n",
    "conditions = [{'encoder.layer4.1': [id], 'y': [0]} for id in concept_ids]\n",
    "\n",
    "heatmap, _, _, _ = attribution(data, conditions, composite_res)\n",
    "\n",
    "imgify(heatmap, symmetric=True, grid=(1, len(concept_ids)))\n",
    "\n"
   ],
   "id": "96e8a411695e7e9b",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T21:34:30.002940Z",
     "start_time": "2024-09-10T21:34:18.968854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [{'encoder.layer4.1': [id], 'y': [0]} for id in torch.arange(0, 512)]\n",
    "\n",
    "for attr in attribution.generate(data, conditions, composite_res, record_layer=layer_names, batch_size=10):\n",
    "    pass\n"
   ],
   "id": "56998ad4ce9f190e",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:35.424098Z",
     "start_time": "2024-09-12T08:44:35.418530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "mask = torch.zeros(224, 224).to(attribution.device)\n",
    "mask[:, 180:] = 1\n",
    "\n",
    "imgify(mask, symmetric=True)\n",
    "\n"
   ],
   "id": "d57ac06cd304b063",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:37.324270Z",
     "start_time": "2024-09-12T08:44:36.418299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.helper import abs_norm\n",
    "\n",
    "rel_c = []\n",
    "for attr in attribution.generate(data, conditions, composite_res, record_layer=layer_names, batch_size=10):\n",
    "    \n",
    "    masked = attr.heatmap * mask[None, :, :]\n",
    "    rel_c.append(torch.sum(masked, dim=(1, 2)))\n",
    "\n",
    "rel_c = torch.cat(rel_c)\n",
    "\n",
    "indices = torch.topk(rel_c, 5).indices\n",
    "# we norm here, so that we clearly see the contribution inside the masked region as percentage\n",
    "indices, abs_norm(rel_c)[indices]*100"
   ],
   "id": "9e1be0f574ebe399",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:38.942710Z",
     "start_time": "2024-09-12T08:44:38.719692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [{\"y\": [0], \"encoder.layer4.1\": [469]}]\n",
    "\n",
    "attr = attribution(data, conditions, composite_res, record_layer=[\"encoder.layer2.1\"])\n",
    "\n",
    "rel_c = cc.attribute(attr.relevances[\"encoder.layer2.1\"], abs_norm=True)\n",
    "\n",
    "# five concepts in features.37 that contributed the most to the activation of channel 469 in features.40\n",
    "# while being relevant for the classification of the lizard class\n",
    "torch.argsort(rel_c, descending=True)[0, :5]\n"
   ],
   "id": "80ee6fb9e4fa5585",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:41.210349Z",
     "start_time": "2024-09-12T08:44:39.920642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.graph import trace_model_graph\n",
    "\n",
    "graph = trace_model_graph(model, data, layer_names)\n",
    "print(graph)"
   ],
   "id": "6b857ffbcef0981",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:42.035983Z",
     "start_time": "2024-09-12T08:44:42.032275Z"
    }
   },
   "cell_type": "code",
   "source": "graph.find_input_layers(\"encoder.layer4.1\")",
   "id": "7f84223678663a32",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:44.262991Z",
     "start_time": "2024-09-12T08:44:42.821485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.attribution import AttributionGraph\n",
    "\n",
    "layer_names = get_layer_names(model, [bottleneck_type])\n",
    "    \n",
    "layer_map = {name: cc for name in layer_names}\n",
    "print(layer_map)\n",
    "attgraph = AttributionGraph(attribution, graph, layer_map)\n",
    "\n",
    "# decompose concept 71 in features.40 w.r.t. target 46 (lizard class)\n",
    "# width=[5, 2] returns first the 5 most relevant concepts in the previous lower-level layer\n",
    "# and in the second iteration returns for each of the 5 most relevant concepts again the two\n",
    "# most relevant concepts in the previous lower-level layer\n",
    "nodes, connections = attgraph(data, composite_res, 71, \"encoder.layer4.1\", 0, width=[5, 2], abs_norm=True)\n",
    "print(\"Nodes:\\n\", nodes, \"\\nConnections:\\n\", connections)\n"
   ],
   "id": "9bd949784cee0c80",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:45.607325Z",
     "start_time": "2024-09-12T08:44:45.603114Z"
    }
   },
   "cell_type": "code",
   "source": "nodes",
   "id": "257174f90dacfdee",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:46.262608Z",
     "start_time": "2024-09-12T08:44:46.258731Z"
    }
   },
   "cell_type": "code",
   "source": "connections[(\"encoder.layer4.1\", 71)]",
   "id": "99c218a4e0f4da2f",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:46.873096Z",
     "start_time": "2024-09-12T08:44:46.867528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "model.to(device)"
   ],
   "id": "f97d520e7387d7e4",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:48.905359Z",
     "start_time": "2024-09-12T08:44:47.479392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../Training_Data/\"\n",
    "train_loader, val_loader = get_data_loaders(data_dir, 64)\n",
    "dataset = get_dataset(data_dir)\n",
    "from torchvision import transforms\n",
    "\n",
    "class STDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, device=\"mps\", transforms=transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            # mean and std of the whole dataset\n",
    "            transforms.Normalize([0.7406, 0.5331, 0.7059], [0.1651, 0.2174, 0.1574])\n",
    "            ])):\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        gene_names = list(self.dataframe)[1:]\n",
    "        gene_vals = []\n",
    "        row = self.dataframe.iloc[index]\n",
    "        a = Image.open(row[\"tile\"]).convert(\"RGB\")\n",
    "        # print(x.size)\n",
    "        for j in gene_names:\n",
    "            gene_val = float(row[j])\n",
    "            gene_vals.append(gene_val)\n",
    "        e = row[\"tile\"]\n",
    "        # apply normalization transforms as for encoder colon classifier\n",
    "        a = self.transforms(a)\n",
    "        a = a.to(self.device)\n",
    "        return a, 0\n",
    "datasetST = STDataset(dataset)"
   ],
   "id": "496b50c055ad79cd",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:44:49.468100Z",
     "start_time": "2024-09-12T08:44:49.462551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cpu\"\n",
    "print(device)\n",
    "print(model.to(device))"
   ],
   "id": "52a9b8031764e869",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:54:56.048930Z",
     "start_time": "2024-09-12T08:44:50.123442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.attribution import CondAttribution\n",
    "from crp.visualization import FeatureVisualization\n",
    "import torchvision.transforms as T\n",
    "\n",
    "cc = ChannelConcept()\n",
    "\n",
    "layer_names = get_layer_names(model, [bottleneck_type])\n",
    "layer_map = {layer : cc for layer in layer_names}\n",
    "model.to(device)\n",
    "print(next(model.parameters()).is_mps)\n",
    "attribution = CondAttribution(model)\n",
    "\n",
    "# separate normalization from resizing for plotting purposes later\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "# apply no normalization here!\n",
    "#imagenet_data = torchvision.datasets.ImageNet(train_loader, transform=transform, split=\"val\")  \n",
    "fv_path = \"output_crp\"\n",
    "fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path)\n",
    "\n",
    "\n",
    "\n",
    "# it will take approximately 20 min on a Titan RTX\n",
    "#print(device)\n",
    "saved_files = fv.run(composite_res, 0, len(datasetST), 124, 100)\n",
    "\n"
   ],
   "id": "1ebda900ca37de52",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "len(datasetST)/64\n",
    "\n"
   ],
   "id": "dc8e572dc7ff2412",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#%matplotlib inline\n",
    "from crp.image import plot_grid\n",
    "\n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"encoder.layer4.1\", \"relevance\", (0, 8))\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 5), padding=False)\n"
   ],
   "id": "2b87e0c695323e25",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:54:58.752163Z",
     "start_time": "2024-09-12T08:54:58.707029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"encoder.layer4.1\", \"relevance\", (0, 8), composite=composite_res, plot_fn=None)\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 9))"
   ],
   "id": "258954acbef2b298",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T22:52:17.935647Z",
     "start_time": "2024-09-10T22:52:13.810926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.image import vis_opaque_img\n",
    "\n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"encoder.layer4.1\", \"relevance\", (0, 8), composite=composite_res, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_c, cmap=\"bwr\", symmetric=True, figsize=(6, 5))\n"
   ],
   "id": "c48f4dd4c47c2f29",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T22:52:21.406413Z",
     "start_time": "2024-09-10T22:52:17.936685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"encoder.layer4.1\", \"relevance\", (0, 8), rf=True, composite=composite_res, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 5), padding=False)\n"
   ],
   "id": "f6fabec7d13d29d3",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T22:52:21.428059Z",
     "start_time": "2024-09-10T22:52:21.407186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "targets, rel = fv.compute_stats(469, \"encoder.layer4.1\", \"relevance\", top_N=5, norm=True)\n",
    "targets, rel \n"
   ],
   "id": "65c81beb7a3f735",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "16d90beb69bc4778"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T22:52:22.357174Z",
     "start_time": "2024-09-10T22:52:21.429382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_t = fv.get_stats_reference(161, \"encoder.layer4.1\", targets, \"relevance\", (0, 8), rf=True, composite=composite_res, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_t, figsize=(6, 5), padding=False)"
   ],
   "id": "ca228c2e49a87813",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T22:52:22.360392Z",
     "start_time": "2024-09-10T22:52:22.357977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.cache import ImageCache\n",
    "\n",
    "cache = ImageCache(path=\"cache\")\n",
    "\n",
    "fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path, cache=cache)\n"
   ],
   "id": "81c30c70039e26fe",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T00:17:41.520045Z",
     "start_time": "2024-09-10T22:52:25.824016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.helper import get_output_shapes\n",
    "import numpy as np\n",
    "\n",
    "layer_names = get_layer_names(model, [bottleneck_type])\n",
    "output_shape = get_output_shapes(model, fv.get_data_sample(0)[0], layer_names)\n",
    "layer_id_map = {l_name: np.arange(0, out[0]) for l_name, out in output_shape.items()}\n",
    "\n",
    "fv.precompute_ref(layer_id_map,  plot_list=[vis_opaque_img], mode=\"relevance\", r_range=(0, 16), composite=composite_res, rf=True, batch_size=32, stats=False)\n"
   ],
   "id": "76d41450afd43384",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "4a8ea53131f426f7",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Feature Visualization\n",
    "\n",
    "In this notebook, we will explore how to visualize which concepts individual CNN channels and MLP neurons encode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Computing Feature Visualizations](#1-computing-feature-visualizations). Visualize concepts using Relevance and Activation Maximization.\n",
    "    * [Conditonal Heatmaps on Reference Samples](#2-conditonal-heatmaps-on-reference-samples). What part of the reference image is actually important for the model?\n",
    "    * [Zooming into the Receptive Field](#3-zooming-into-the-receptive-field). Compute the receptive field of neurons inside CNN layers and use it to crop out the most relevant part of the reference sample to increase interpretability even further\n",
    "* [Statistics](#4-statistics). See how concepts behave for different classes.\n",
    "* [Caching and Precomputing Reference Images](#5-caching-and-precomputing-reference-images). Cache and precompute reference images for optimal performance in low-latency applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing Feature Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start again by initializing the VGG16 model and the image of the lizard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = vgg16_bn(True).to(device)\n",
    "model.eval()\n",
    "\n",
    "canonizers = [SequentialMergeBatchNorm()]\n",
    "composite = EpsilonPlusFlat(canonizers)\n",
    "\n",
    "image = Image.open(\"images/lizard.jpg\")\n",
    "image"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FeatureVisualization` class contains the functionality to visualize concepts. For this, we must first define what kind of concept is used in which layer of the model by initializing a _layer_map_ dictionary. Moreover, we need a dataset through which we iterate and a `CondAttribution` object that describes how attributions should be computed on the dataset. It is important not to apply any preprocessing to the images yet. This will be done later in the `FeatureVisualization` object, so that the built-in plot functions can be used. Otherwise you would have to write your own plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.attribution import CondAttribution\n",
    "from crp.visualization import FeatureVisualization\n",
    "from tutorials.VGG16_ImageNet.download_imagenet import download\n",
    "\n",
    "cc = ChannelConcept()\n",
    "\n",
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "layer_map = {layer : cc for layer in layer_names}\n",
    "\n",
    "attribution = CondAttribution(model)\n",
    "\n",
    "# separate normalization from resizing for plotting purposes later\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "from script.data_processing.image_transforms import get_eval_transforms\n",
    "preprocessing =  get_eval_transforms(image_size=224)\n",
    "\n",
    "data_path = None #TODO fill or run download below\n",
    "\n",
    "if data_path is None:\n",
    "    # download ImageNet validation set\n",
    "    data_path = \"ImageNet_data\"\n",
    "    download(data_path)\n",
    "    \n",
    "# apply no normalization here!\n",
    "imagenet_data = torchvision.datasets.ImageNet(data_path, transform=transform, split=\"val\")  "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize the `FeatureVisualization` class with a custom preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "fv_path = \"VGG16_ImageNet\"\n",
    "fv = FeatureVisualization(attribution, imagenet_data, layer_map, preprocess_fn=preprocessing, path=fv_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now run the analysis or skip it by going to the next line of Python code (the analysis results for layer features.40 were precomputed and are located inside the tutorials/VGG16_ImageNet directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# it will take approximately 20 min on a Titan RTX\n",
    "saved_files = fv.run(composite, 0, len(imagenet_data), 32, 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we discovered the 5 most important concepts in classifying the lizard. Now let's visualize the 8 most representative input samples for each of these concepts using Relevance Maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from crp.image import plot_grid\n",
    "\n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8))\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 5), padding=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_grid` function converts tensors of images and heatmaps to `PIL.Image` files automatically using the `crp.imgify` function. Thus, you will find all necessary paramters of `crp.imgify` also in `plot_grid`. In addition, `plot_grid` is capable of visualizing the types Dict[Any, List[Arrays]] as well as Dict[Any, Tuple[List[Arrays], List[Arrays]]]. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conditonal Heatmaps on Reference Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The room for interpretation in the pictures is large. One does not know exactly which part to focus on. Therefore, we can compute conditional heatmaps for the concepts. The intern `attribution` method of the `CondAttribution` class starts the backward pass directly at the layer of the concept and initializes with the activation of the channel, so that we achieve a localization of the concept in input space. The method `get_max_reference` returns then the reference samples as well as attribution heatmaps as separate lists in the dictionary. More details on the `plot_fn` argument is coming soon.\n",
    "\n",
    "To achieve this, we simply supply a zennit composite to the `composite` argument describing the rule used for computing attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8), composite=composite, plot_fn=None)\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 9))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manner in which the reference images are plotted is controlled in the `plot_fn` argument of the `get_max_reference` method. If the parameter is None, the original tensors are returned. This function is given three arguments: unpreprocessed datasamples, the computed heatmap and a boolean `rf` (more about this in a moment). \n",
    "If you want to write your own plot function, you should follow this signature.\n",
    "For example, to merge the heatmap into the reference image by utilizing it as an opaque mask to hide insignificant parts, we could use the prebuilt function `vis_opaque_img`.\n",
    "\n",
    "However, by default, the `plot_fn` is set to `vis_img_heatmap`, which is simply applying `crp.imgify` to all elements and cropping the receptive field (more about this in a moment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "from crp.image import vis_opaque_img\n",
    "\n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8), composite=composite, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_c, cmap=\"bwr\", symmetric=True, figsize=(6, 5))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Zooming into the Receptive Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D-Conv layers are made up of an 2D-array of neurons with identical weights i.e. filters.\n",
    "Instead of visualizing the whole input sample and its whole 2D-channel heatmap, we could focus on a single neuron, a single filter - the atomic unit - of a convolutional channel. By following the receptive field of the most relevant neuron inside a 2D-channel, we can zoom into the input sample and highlight the most important part of it. (The `FeatureVisualization` class automatically computes the most relevant neuron index during the analysis.)\n",
    "\n",
    "For that, we must set the `rf` argument of the `get_max_reference` method to True and at the same time supply a zennit composite to the `composite` argument. Behind the scenes, the method computes an attribution starting at the most relevant neuron while initializing the relevance at all other neurons with zero. Inside the `plot_fn` the receptive field is then used to crop the image. (The cropping threshold and other hyperparameters of the `vis_opaque_img` or `vis_img_heatmap` function can be tuned wrt. your specific dataset.)\n",
    "\n",
    "The `padding` argument specifies whether padding should be added to the reference images, just play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8), rf=True, composite=composite, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_c, figsize=(6, 5), padding=False)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library also calculates statistics about the average activation or relevance of a concept. This allows us to determine for which class this concept is most often used and which reference images in each class are most representative. This increases the diversity of the reference images, which is beneficial for interpretability.\n",
    "\n",
    "The method `compute_stats` returns the most representative classes i.e. targets and their mean values. If you set the `norm` parameter, the values are normed wrt. the maximal value. Use the `top_N` parameter to return the top-N targets.\n",
    "\n",
    "If you did not run the analysis on your own, you can only visualize the example below. Providing files for alle concepts and layers consumes too much disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "targets, rel = fv.compute_stats(469, \"features.40\", \"relevance\", top_N=5, norm=True)\n",
    "targets, rel "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `get_max_reference`, the method `get_stats_reference` returns PIL Images that can be plotted - depending on the `plot_fn` you supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "ref_t = fv.get_stats_reference(469, \"features.40\", targets, \"relevance\", (0, 8), rf=True, composite=composite, plot_fn=vis_opaque_img)\n",
    "\n",
    "plot_grid(ref_t, figsize=(6, 5), padding=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, that for concept 469 the targets 46, 40, 42, 41 and 44 are most relevant. All classes show amphibian characteristics, thus channel 469 is specialized in detecting amphibian features - especially the legs of lizards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Caching and Precomputing Reference Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageCache` provides the functionality to save and load single PIL Image files as they are found inside the _ref_ dictionary. For that, a path must be defined during instantiation and the object passed to the `FeatureVisualization` object inside the `cache` argument.\n",
    "To construct a custom cache, you have to inherit the `Cache` class and only overwrite the `save` and `load` method. The `save` method is automatically called after visualizing the sample in the `plot_fn` function and thus saves its output values. On the other hand, the `load` method is called before computing the reference images and loads the files if available. This applies to both methods: `get_max_reference` and `get_stats_reference`.\n",
    "\n",
    "Note, that `ImageCache` is able to save lists of PIL.Images as returned by `vis_opaque_img` and tuples of two lists of PIL.Images as returned by `vis_img_heatmap`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "from crp.cache import ImageCache\n",
    "\n",
    "cache = ImageCache(path=\"cache\")\n",
    "\n",
    "fv = FeatureVisualization(attribution, imagenet_data, layer_map, preprocess_fn=preprocessing, path=fv_path, cache=cache)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the wall time for computing the reference images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "%%time \n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8), rf=True, composite=composite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then again after they were cached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "%%time \n",
    "ref_c = fv.get_max_reference([469, 35, 89, 316, 161], \"features.40\", \"relevance\", (0, 8), rf=True, composite=composite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling `get_max_reference` for all concepts to fill the cache with images, we could precompute all reference images.\n",
    "\n",
    "To do this, we call the `precompute_ref` method of the `FeatureVisualization` object and supply a dictionary that contains as keys layer names and as values the indices of concepts we'd like to compute. For convenience, we use the helper method `get_output_shapes` that traces the output of our model and returns the output shapes of our layers without batch dimensions as dictionary. Then, we use the dictionary to infer how many concepts we have in each layer.\n",
    "\n",
    "The `plot_list` argument allows us to define several plot_fn with which we visualize the images and the `stats` argument computes reference images also for the `get_stats_reference` method, which will result in much larger disk space consumption. Depending on the selected parameters such as `composite` or `rf`, the images are saved separately in different folders. Just take a look into the cached files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "from crp.helper import get_output_shapes\n",
    "import numpy as np\n",
    "\n",
    "layer_names = get_layer_names(model, [torch.nn.Conv2d])\n",
    "output_shape = get_output_shapes(model, fv.get_data_sample(0)[0], layer_names)\n",
    "layer_id_map = {l_name: np.arange(0, out[0]) for l_name, out in output_shape.items()}\n",
    "\n",
    "fv.precompute_ref(layer_id_map,  plot_list=[vis_opaque_img], mode=\"relevance\", r_range=(0, 16), composite=composite, rf=True, batch_size=32, stats=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precomputing 32 reference images per concept for RelevanceMaximization in all convolutional layers of the VGG16 model consumes approximately 1.2 GB of disk space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tool')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2454eddf0b216369ddcaa6c1a78b4d7c10611a9506483fadb2b8cad3cc9934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

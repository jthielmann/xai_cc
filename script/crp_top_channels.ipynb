{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-14T11:38:17.243091Z"
    }
   },
   "source": [
    "from crp.attribution import CondAttribution\n",
    "from zennit.attribution import Gradient\n",
    "from zennit.canonizers import CompositeCanonizer\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.visualization import FeatureVisualization\n",
    "from crp.image import plot_grid\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "import zennit as zen\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import zennit.torchvision as ztv\n",
    "from crp.image import imgify\n",
    "\n",
    "\n",
    "from relevance import plot_relevance\n",
    "from model import get_vggs_and_path, get_resnets_and_path, get_remote_models_and_path\n",
    "from plot_and_print import plot_tile\n",
    "from data_loader import TileLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from data_loader import get_data_loaders, get_dataset, STDataset\n",
    "\n",
    "\n",
    "import torchvision.transforms as T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T06:50:17.106278Z",
     "start_time": "2024-09-12T06:50:14.046778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = get_remote_models_and_path()\n",
    "img_paths = []\n",
    "img_paths.append(\"../Test_Data/p026/Tiles_156/p026_11_60.tiff\")\n",
    "\n",
    "loader = TileLoader()\n",
    "for path in img_paths:\n",
    "    data = loader.open(path).unsqueeze(0)\n",
    "    data.requires_grad_(True)\n",
    "    plot_tile(path)\n",
    "    \n",
    "cc = ChannelConcept()\n",
    "\n",
    "data_dir = \"../Training_Data/\"\n",
    "dataset = get_dataset(data_dir)\n",
    "from torchvision import transforms\n",
    "\n",
    "class STDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, device=\"mps\", transforms=transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            # mean and std of the whole dataset\n",
    "            transforms.Normalize([0.7406, 0.5331, 0.7059], [0.1651, 0.2174, 0.1574])\n",
    "            ])):\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        gene_names = list(self.dataframe)[1:]\n",
    "        gene_vals = []\n",
    "        row = self.dataframe.iloc[index]\n",
    "        a = Image.open(row[\"tile\"]).convert(\"RGB\")\n",
    "        # print(x.size)\n",
    "        for j in gene_names:\n",
    "            gene_val = float(row[j])\n",
    "            gene_vals.append(gene_val)\n",
    "        e = row[\"tile\"]\n",
    "        # apply normalization transforms as for pretrained colon classifier\n",
    "        a = self.transforms(a)\n",
    "        a = a.to(self.device)\n",
    "        return a, 0\n",
    "datasetST = STDataset(dataset)"
   ],
   "id": "ba681e78c24ac36a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T06:41:12.421555Z",
     "start_time": "2024-09-12T06:41:12.102997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model, model_path in models:\n",
    "    for path in img_paths:\n",
    "        data = loader.open(path).unsqueeze(0)\n",
    "        data.requires_grad_(True)\n",
    "        #print(type(model.pretrained))\n",
    "        if type(model.pretrained).__name__ == \"VGG\":\n",
    "            composite = zen.composites.EpsilonPlusFlat(canonizers=[ztv.VGGCanonizer()])\n",
    "        else:\n",
    "            composite = zen.composites.EpsilonPlusFlat(canonizers=[ztv.ResNetCanonizer()])\n",
    "            \n",
    "        print(\"selected\", type(composite.canonizers[0]).__name__, \"for model\", model_path)\n",
    "        attribution = CondAttribution(model, no_param_grad=True)\n",
    "        #print(model)\n",
    "        if type(model.pretrained).__name__ == \"VGG\":\n",
    "            layer_type = model.pretrained.classifier[-1].__class__\n",
    "            print(model.pretrained.classifier[-1])\n",
    "            layer_name = get_layer_names(model, [nn.Linear])[-3]\n",
    "        else:\n",
    "            layer_type = model.pretrained.layer1[0].__class__\n",
    "            # select last bottleneck module\n",
    "            layer_name = get_layer_names(model, [layer_type])[-1]\n",
    "        #print(layer_type)\n",
    "        \n",
    "        print(layer_name)\n",
    "        conditions = [{'y': [0]}]\n",
    "        attr = attribution(data, conditions, composite, record_layer=[layer_name])\n",
    "        \n",
    "        #print(attr.activations[layer_name].shape, attr.relevances[layer_name].shape)\n",
    "        # attr[1][\"features.40\"].shape, attr[2][\"features.40\"].shape # is equivalent\n",
    "        rel_c = cc.attribute(attr.relevances[layer_name], abs_norm=True)\n",
    "        #print(rel_c.shape)\n",
    "        #print(rel_c)\n",
    "        rel_values, concept_ids = torch.topk(rel_c[0], 7)\n",
    "        print(concept_ids, rel_values*100)\n",
    "        position = path.find('models') + len('models')\n",
    "        base_path = os.path.basename(model_path[position:-3])\n",
    "        print(base_path)\n",
    "        if os.path.exists(base_path):\n",
    "            print(base_path, \"already exists, continue..\")\n",
    "            fv_path = \"output_crp_\" + os.path.basename(model_path[position:-3])\n",
    "            cc = ChannelConcept()\n",
    "    \n",
    "            layer_names = get_layer_names(model, [layer_type])\n",
    "            layer_map = {layer : cc for layer in layer_names}\n",
    "            preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path)\n",
    "            #saved_files = fv.run(composite, 0, len(datasetST), 32, 100)\n",
    "            ref_c = fv.get_max_reference(concept_ids, \"pretrained.layer4.1\", \"relevance\", (0, 8), composite=composite, plot_fn=None)\n",
    "    \n",
    "            plot_grid(ref_c, figsize=(6, 9))\n",
    "            continue\n",
    "        os.mkdir(\"./\" + base_path)\n",
    "        fv_path = \"output_crp_\" + os.path.basename(model_path[position:-3])\n",
    "        cc = ChannelConcept()\n",
    "\n",
    "        layer_names = get_layer_names(model, [layer_type])\n",
    "        layer_map = {layer : cc for layer in layer_names}\n",
    "        preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path)\n",
    "        saved_files = fv.run(composite, 0, len(datasetST), 32, 100)\n",
    "        ref_c = fv.get_max_reference(concept_ids, \"pretrained.layer4.1\", \"relevance\", (0, 8), composite=composite, plot_fn=None)\n",
    "\n",
    "        plot_grid(ref_c, figsize=(6, 9))\n",
    "    "
   ],
   "id": "8ff1803af6bc044b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T09:31:51.007676Z",
     "start_time": "2024-09-14T09:31:50.896909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model, model_path in models:\n",
    "    for path in img_paths:\n",
    "        data = loader.open(path).unsqueeze(0)\n",
    "        data.requires_grad_(True)\n",
    "        #print(type(model.pretrained))\n",
    "        if type(model.pretrained).__name__ == \"VGG\":\n",
    "            composite = zen.composites.EpsilonPlusFlat(canonizers=[ztv.VGGCanonizer()])\n",
    "        else:\n",
    "            composite = zen.composites.EpsilonPlusFlat(canonizers=[ztv.ResNetCanonizer()])\n",
    "            \n",
    "        print(\"selected\", type(composite.canonizers[0]).__name__, \"for model\", model_path)\n",
    "        attribution = CondAttribution(model, no_param_grad=True)\n",
    "        #print(model)\n",
    "        if type(model.pretrained).__name__ == \"VGG\":\n",
    "            layer_type = model.pretrained.classifier[-1].__class__\n",
    "            print(model.pretrained.classifier[-1])\n",
    "            layer_name = get_layer_names(model, [nn.Linear])[-3]\n",
    "        else:\n",
    "            layer_type = model.pretrained.layer1[0].__class__\n",
    "            # select last bottleneck module\n",
    "            layer_name = get_layer_names(model, [layer_type])[-1]\n",
    "        #print(layer_type)\n",
    "        \n",
    "        #print(layer_name)\n",
    "        conditions = [{'y': [0]}]\n",
    "        attr = attribution(data, conditions, composite, record_layer=[layer_name])\n",
    "        \n",
    "        #print(attr.activations[layer_name].shape, attr.relevances[layer_name].shape)\n",
    "        # attr[1][\"features.40\"].shape, attr[2][\"features.40\"].shape # is equivalent\n",
    "        rel_c = cc.attribute(attr.relevances[layer_name], abs_norm=True)\n",
    "        #print(rel_c.shape)\n",
    "        #print(rel_c)\n",
    "        rel_values, concept_ids = torch.topk(rel_c[0], 7)\n",
    "        print(concept_ids, rel_values*100)\n",
    "        position = model_path.find('/models/') + len('/models/')\n",
    "        base_path = \"../crp_out/\" + model_path[position:-3]\n",
    "        \n",
    "        #if the file exists then just plot\n",
    "        if os.path.exists(base_path):\n",
    "            print(model_path)\n",
    "            #print(base_path, \"already exists, continue..\")\n",
    "            fv_path = \"output_crp_\" + os.path.basename(model_path[position:-3])\n",
    "            cc = ChannelConcept()\n",
    "    \n",
    "            layer_names = get_layer_names(model, [layer_type])\n",
    "            layer_map = {layer : cc for layer in layer_names}\n",
    "            preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path)\n",
    "            #saved_files = fv.run(composite, 0, len(datasetST), 32, 100)\n",
    "            ref_c = fv.get_max_reference(concept_ids, layer_names[-1], \"relevance\", (0, 8), composite=composite, plot_fn=None)\n",
    "    \n",
    "            plot_grid(ref_c, figsize=(6, 9))\n",
    "            continue\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "        fv_path = base_path\n",
    "        cc = ChannelConcept()\n",
    "\n",
    "        layer_names = get_layer_names(model, [layer_type])\n",
    "        layer_map = {layer : cc for layer in layer_names}\n",
    "        preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        fv = FeatureVisualization(attribution, datasetST, layer_map, preprocess_fn=preprocessing, path=fv_path)\n",
    "        saved_files = fv.run(composite, 0, len(datasetST), 32, 100)\n",
    "        ref_c = fv.get_max_reference(concept_ids, \"pretrained.layer4.1\", \"relevance\", (0, 8), composite=composite, plot_fn=None)\n",
    "\n",
    "        plot_grid(ref_c, figsize=(6, 9))\n",
    "        plt.show()"
   ],
   "id": "397a816ec69c3436",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, model_path \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodels\u001B[49m:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m img_paths:\n\u001B[1;32m      3\u001B[0m         data \u001B[38;5;241m=\u001B[39m loader\u001B[38;5;241m.\u001B[39mopen(path)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'models' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "e1029b0ae0108cd8",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:03:16.377020Z",
     "start_time": "2024-08-26T13:03:16.374754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from crp.attribution import CondAttribution\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.visualization import FeatureVisualization\n",
    "from crp.image import plot_grid\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from model import get_models_and_path\n",
    "from data_loader import TileLoader\n",
    "import os\n",
    "import pandas as pd"
   ],
   "id": "48e3d1fbc8de2692",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:09:40.686689Z",
     "start_time": "2024-08-26T13:09:36.217387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = get_models_and_path()\n",
    "loader = TileLoader()"
   ],
   "id": "820bc5a8d0f42563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:10:30.956217Z",
     "start_time": "2024-08-26T13:10:30.953244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = models[0][0]\n",
    "\n",
    "# we need to set the flag so the grad calculations later work properly\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.gene1.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.pretrained.parameters():\n",
    "    param.requires_grad = True"
   ],
   "id": "5d5dfa13fc64e16",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:03:22.540563Z",
     "start_time": "2024-08-26T13:03:22.523810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../results/\"\n",
    "patients = [os.path.basename(f) for f in os.scandir(data_dir) if f.is_dir()]\n",
    "patients.remove('p021')\n",
    "filenames = []\n",
    "for patient in patients:\n",
    "    filenames.append(data_dir + patient + \"/RUBCNL_results.csv\")\n",
    "\n",
    "df = pd.read_csv(filenames[0])\n",
    "def cut_path(x):\n",
    "    return x[3:]\n",
    "new_col = df.path\n",
    "new_col = new_col.apply(cut_path)\n",
    "df.path = new_col\n",
    "\n",
    "loader = TileLoader()"
   ],
   "id": "ff9657d75efa4550",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:03:55.207646Z",
     "start_time": "2024-08-26T13:03:55.200337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sorted_values_by_col(df, min, max, colname='../models/res18/RUBCNL_Res18/Res18_1000_ep_29.pt', gene=\"RUBCNL\"):\n",
    "    idx = df[gene].sort_values()[min:max].index.values\n",
    "\n",
    "    vals_orig_by_range = df.iloc[idx]\n",
    "    diff = vals_orig_by_range[gene] - vals_orig_by_range[colname]\n",
    "\n",
    "    diff = diff.abs().sort_values()\n",
    "    return df.iloc[diff.index]\n",
    "small_vals  = get_sorted_values_by_col(df, 0, 100)\n",
    "middle_vals = get_sorted_values_by_col(df, int(len(df)/2-50), int(len(df)/2+50))\n",
    "big_vals    = get_sorted_values_by_col(df, -100, len(df))"
   ],
   "id": "e5ff51e14ce60c45",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:06:50.270242Z",
     "start_time": "2024-08-26T13:06:50.247780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best = [small_vals[0:5], middle_vals[0:5], big_vals[0:5]]\n",
    "worst = [small_vals[-5:], middle_vals[-5:], big_vals[-5:]]\n",
    "# interesting:\n",
    "interesting = []\n",
    "interesting.append(best[0].index.values[1])\n",
    "#set = worst[2]\n",
    "#id = set.index.values[1]\n",
    "tile_set = best[2]\n",
    "tile_id = tile_set.index.values[1]\n",
    "data = loader.open(df.path[tile_id]).unsqueeze(0)"
   ],
   "id": "96ac77500c938e96",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T13:10:34.465617Z",
     "start_time": "2024-08-26T13:10:34.434573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define LRP rules and canonizers in zennit\n",
    "composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "\n",
    "# load CRP toolbox\n",
    "attribution = CondAttribution(model)\n",
    "\n",
    "# here, each channel is defined as a concept\n",
    "# or define your own notion!\n",
    "cc = ChannelConcept()\n",
    "\n",
    "# get layer names of Conv2D and MLP layers\n",
    "layer_names = get_layer_names(model, [nn.Conv2d, nn.Linear])\n",
    "\n",
    "# get a conditional attribution for channel 50 in layer features.27 wrt. output 1\n",
    "conditions = [{'features.27': [50], 'y': [1]}]\n",
    "\n",
    "attr = attribution(data, conditions, composite, record_layer=layer_names)\n",
    "\n",
    "# heatmap and prediction\n",
    "attr.heatmap, attr.prediction\n",
    "# activations and relevances for each layer name\n",
    "attr.activations, attr.relevances\n",
    "\n",
    "# relative importance of each concept for final prediction\n",
    "rel_c = cc.attribute(attr.relevances['features.40'])\n",
    "# most relevant channels in features.40\n",
    "concept_ids = torch.argsort(rel_c, descending=True)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "can't retain_grad on Tensor that has requires_grad=False",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [20], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# get a conditional attribution for channel 50 in layer features.27 wrt. output 1\u001B[39;00m\n\u001B[1;32m     15\u001B[0m conditions \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures.27\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m50\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m]}]\n\u001B[0;32m---> 17\u001B[0m attr \u001B[38;5;241m=\u001B[39m \u001B[43mattribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomposite\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecord_layer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# heatmap and prediction\u001B[39;00m\n\u001B[1;32m     20\u001B[0m attr\u001B[38;5;241m.\u001B[39mheatmap, attr\u001B[38;5;241m.\u001B[39mprediction\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/crp/attribution.py:239\u001B[0m, in \u001B[0;36mCondAttribution.__call__\u001B[0;34m(self, data, conditions, composite, record_layer, mask_map, start_layer, init_rel, on_device, exclude_parallel)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03mComputes conditional attributions by masking the gradient flow of PyTorch (that is replaced by zennit with relevance values).\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;124;03mThe relevance distribution rules (as for LRP e.g.) are described in the zennit 'composite'. Relevance can be initialized at\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;124;03m        The model prediction output. If 'start_layer' is set, 'prediction' is the layer activation.       \u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exclude_parallel:\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conditions_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomposite\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecord_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_rel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_attribute(data, conditions, composite, record_layer, mask_map, start_layer, init_rel, on_device, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/crp/attribution.py:258\u001B[0m, in \u001B[0;36mCondAttribution._conditions_wrapper\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    254\u001B[0m dist_conds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_separate_conditions(conditions)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dist_layer \u001B[38;5;129;01min\u001B[39;00m dist_conds:\n\u001B[0;32m--> 258\u001B[0m     attr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attribute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdist_conds\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdist_layer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m l_name \u001B[38;5;129;01min\u001B[39;00m attr\u001B[38;5;241m.\u001B[39mrelevances:\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m l_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m relevances:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/crp/attribution.py:304\u001B[0m, in \u001B[0;36mCondAttribution._attribute\u001B[0;34m(self, data, conditions, composite, record_layer, mask_map, start_layer, init_rel, on_device, exclude_parallel)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_attribute\u001B[39m(\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;28mself\u001B[39m, data: torch\u001B[38;5;241m.\u001B[39mtensor, conditions: List[Dict[\u001B[38;5;28mstr\u001B[39m, List]],\n\u001B[1;32m    296\u001B[0m         composite: Composite \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, record_layer: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m [],\n\u001B[1;32m    297\u001B[0m         mask_map: Union[Callable, Dict[\u001B[38;5;28mstr\u001B[39m, Callable]] \u001B[38;5;241m=\u001B[39m ChannelConcept\u001B[38;5;241m.\u001B[39mmask, start_layer: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, init_rel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    298\u001B[0m         on_device: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, exclude_parallel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m attrResult:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;124;03m    Computes the actual attributions as described in __call__ method docstring.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;124;03m    exclude_parallel: boolean\u001B[39;00m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;124;03m        If set, all layer names in 'conditions' must be identical. This limitation does not apply to the __call__ method.\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m     data, conditions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_arguments(data, conditions, start_layer, exclude_parallel, init_rel)\n\u001B[1;32m    308\u001B[0m     hook_map, y_targets, cond_l_names \u001B[38;5;241m=\u001B[39m {}, [], []\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/crp/attribution.py:126\u001B[0m, in \u001B[0;36mCondAttribution.broadcast\u001B[0;34m(self, data, conditions)\u001B[0m\n\u001B[1;32m    123\u001B[0m len_data, len_cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data), \u001B[38;5;28mlen\u001B[39m(conditions)\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m len_data \u001B[38;5;241m==\u001B[39m len_cond:\n\u001B[0;32m--> 126\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretain_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data, conditions\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m len_cond \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: can't retain_grad on Tensor that has requires_grad=False"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# define which concept is used in each layer\n",
    "layer_map = {name: cc for name in layer_names}\n",
    "\n",
    "# compute visualization (it takes for VGG16 and ImageNet testset on Titan RTX 30 min)\n",
    "fv = FeatureVisualization(attribution, dataset, layer_map)\n",
    "fv.run(composite, 0, len(dataset))\n",
    "\n",
    "# visualize MaxRelevance reference images for top-5 concepts\n",
    "ref_c = fv.get_max_reference(concept_ids[:5], 'features.40', 'relevance', composite=composite)\n",
    "\n",
    "plot_grid(ref_c)"
   ],
   "id": "b93edafe1f4bd9a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = \"\"\n",
    "loader = \"\"\n",
    "for tile in loader:\n",
    "    generate_heatmap_to_file(tile)\n"
   ],
   "id": "16271b4905f8f1e0",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "canvas = \"blank\"\n",
    "for tile in folder:\n",
    "    canvas[a,b] = tile"
   ],
   "id": "6367165488199ff2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "# plot canvas",
   "id": "706e267820b9ed8e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "# plot canvas after normalize",
   "id": "f18b6d539e72040e",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

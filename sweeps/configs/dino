project: dino
name: train dinov3 on NCT-CRC-HE-100K
method: grid
metric:
  name: valid_loss
  goal: minimize

parameters:
  # Run behavior
  log_to_wandb:
    value: true
  debug:
    value: true
  sanity_run:
    value: false
  use_early_stopping:
    value: true
  generate_scatters:
    value: true

  # Data / loader
  dataset:
    value: NCT-CRC-HE-100K
  batch_size:
    value: 32
  num_workers:
    value: 2
  image_size_global:
    values: [224, 336]        # match your tile scale
  image_size_local:
    values: [96, 112]
  mean:
    value: [0.7406, 0.5331, 0.7059]
  std:
    value: [0.1651, 0.2174, 0.1574]

  # Encoder (DINOv3)
  model_id:
    values:
      - facebook/dinov3-vitb16-pretrain-lvd1689m
  use_hf_normalize:
    values: [true, false]     # toggle to compare HF normalization vs your transforms
  use_stain_norm:
    values: [false, true]     # optional stain normalization before augs
  stain_norm_method:
    value: reinhard
  stain_target_means_lab:
    value: [50.0, 0.0, 0.0]
  stain_target_stds_lab:
    value: [15.0, 5.0, 5.0]
  # Backbone training controls (explicit; no code defaults)
  grad_checkpointing:
    value: true               # enable gradient checkpointing to save VRAM
  unfreeze_last_n:
    value: -1                 # -1=train all; 0=freeze all; N=last N blocks
  backbone_lr_mult:
    value: 1.0                # scales backbone LR vs head LR (used if backbone_lr is null)
  backbone_lr:
    value: 0.0005             # explicit backbone LR; overrides backbone_lr_mult when set

  # Training schedule
  epochs:
    value: 40
  optimizer:
    value: adamw
  learning_rate:
    values: [5e-4, 1e-3]      # scale with effective batch size
  weight_decay:
    values: [0.05, 0.1]
  weight_decay_cosine:
    values: [false, true]
  weight_decay_end:
    value: 0.0
  warmup_epochs:
    value: 10
  lr_schedule:
    value: cosine

  # DINO-specific (teacher/student)
  head_hidden_dim:
    value: 512
  head_bottleneck_dim:
    value: 64
  head_output_dim:
    values: [1024, 2048]
  freeze_last_layer_epochs:
    value: 1
  num_global_views:
    value: 2
  n_local_views:
    value: 6
  global_crop_scale:
    values:
      - [0.4, 1.0]
      - [0.5, 1.0]
      - [0.6, 1.0]
  local_crop_scale:
    value: [0.05, 0.4]

  # Temperatures / centering / EMA
  teacher_temp_warmup:
    value: 0.04
  teacher_temp_final:
    values: [0.07, 0.08]
  student_temp:
    value: 0.1
  center_momentum:
    value: 0.9
  teacher_temp_per_step:
    values: [false, true]
  warmup_teacher_temp_steps:
    value: 0              # 0 = auto from epochs * steps/epoch
  ema_momentum_start:
    value: 0.996
  ema_momentum_end:
    value: 0.9995

  # Augmentations (histology-friendly)
  cj_strength:
    values: [0.3, 0.5, 0.8]
  gaussian_blur:              # [prob_global1, prob_global2, prob_local]
    values:
      - [1.0, 0.1, 0.1]
      - [1.0, 0.1, 0.5]
  solarization_prob:
    values: [0.0, 0.2]
  use_ref_aug_policy:
    values: [false, true]

  # Logging / metrics
  loss_fn_switch:
    value: DinoLoss
  error_metric_name:
    value: DinoLoss

  # Runtime (optional but handy)
  precision_auto:
    value: true               # your code maps this to bf16/16/32 at runtime
  ddp_find_unused_parameters_false:
    value: true               # safe for DINO; faster than scanning for unused params
  use_fused_adamw:
    values: [false, true]     # requires CUDA; safe fallback on CPU
